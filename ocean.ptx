//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21112126
// Cuda compilation tools, release 8.0, V8.0.43
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	buildFrequencyDataKernel
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry buildFrequencyDataKernel(
	.param .u64 buildFrequencyDataKernel_param_0,
	.param .u64 buildFrequencyDataKernel_param_1,
	.param .u64 buildFrequencyDataKernel_param_2,
	.param .u32 buildFrequencyDataKernel_param_3,
	.param .u32 buildFrequencyDataKernel_param_4,
	.param .u32 buildFrequencyDataKernel_param_5,
	.param .u32 buildFrequencyDataKernel_param_6,
	.param .f32 buildFrequencyDataKernel_param_7
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [buildFrequencyDataKernel_param_0];
	ld.param.u64 	%rd2, [buildFrequencyDataKernel_param_1];
	ld.param.u64 	%rd3, [buildFrequencyDataKernel_param_2];
	ld.param.u32 	%r3, [buildFrequencyDataKernel_param_3];
	ld.param.u32 	%r4, [buildFrequencyDataKernel_param_4];
	ld.param.u32 	%r5, [buildFrequencyDataKernel_param_5];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r7, %r6, %r8;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r9, %r10, %r11;
	setp.lt.u32	%p1, %r1, %r4;
	setp.lt.u32	%p2, %r2, %r5;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB0_2;
	bra.uni 	BB0_1;

BB0_1:
	mad.lo.s32 	%r12, %r2, %r4, %r1;
	shr.u32 	%r13, %r3, 1;
	add.s32 	%r14, %r1, %r13;
	rem.u32 	%r15, %r14, %r3;
	add.s32 	%r16, %r2, %r13;
	rem.u32 	%r17, %r16, %r3;
	add.s32 	%r18, %r3, -1;
	sub.s32 	%r19, %r18, %r17;
	cvta.to.global.u64 	%rd4, %rd2;
	add.s32 	%r20, %r15, 1;
	mul.wide.u32 	%rd5, %r20, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.u32 	%rd8, %r19, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd6];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.u32 	%rd11, %r12, 8;
	add.s64 	%rd12, %rd10, %rd11;
	mul.f32 	%f4, %f3, 0f42C80000;
	st.global.v2.f32 	[%rd12], {%f4, %f4};

BB0_2:
	ret;
}

	// .globl	buildFrequencyDataKernel2
.visible .entry buildFrequencyDataKernel2(
	.param .u64 buildFrequencyDataKernel2_param_0,
	.param .u64 buildFrequencyDataKernel2_param_1,
	.param .u64 buildFrequencyDataKernel2_param_2,
	.param .u32 buildFrequencyDataKernel2_param_3,
	.param .u32 buildFrequencyDataKernel2_param_4,
	.param .u32 buildFrequencyDataKernel2_param_5,
	.param .u32 buildFrequencyDataKernel2_param_6,
	.param .f32 buildFrequencyDataKernel2_param_7
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [buildFrequencyDataKernel2_param_0];
	ld.param.u64 	%rd2, [buildFrequencyDataKernel2_param_1];
	ld.param.u64 	%rd3, [buildFrequencyDataKernel2_param_2];
	ld.param.u32 	%r4, [buildFrequencyDataKernel2_param_3];
	ld.param.u32 	%r5, [buildFrequencyDataKernel2_param_4];
	ld.param.u32 	%r6, [buildFrequencyDataKernel2_param_5];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r8, %r7, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.lt.u32	%p1, %r1, %r5;
	setp.lt.u32	%p2, %r2, %r6;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB1_6;
	bra.uni 	BB1_1;

BB1_1:
	mad.lo.s32 	%r3, %r2, %r5, %r1;
	shr.u32 	%r13, %r4, 1;
	add.s32 	%r14, %r1, %r13;
	rem.u32 	%r15, %r14, %r4;
	add.s32 	%r16, %r2, %r13;
	rem.u32 	%r17, %r16, %r4;
	add.s32 	%r18, %r4, -1;
	sub.s32 	%r19, %r18, %r17;
	cvta.to.global.u64 	%rd4, %rd2;
	add.s32 	%r20, %r15, 1;
	mul.wide.u32 	%rd5, %r20, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.u32 	%rd8, %r19, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f5, [%rd9];
	ld.global.f32 	%f6, [%rd6];
	mul.f32 	%f7, %f6, %f5;
	mul.f32 	%f1, %f7, 0f41200000;
	setp.gt.f32	%p4, %f1, 0f3F800000;
	@%p4 bra 	BB1_4;
	bra.uni 	BB1_2;

BB1_4:
	sqrt.rn.f32 	%f11, %f1;
	bra.uni 	BB1_5;

BB1_2:
	setp.geu.f32	%p5, %f1, 0fBF800000;
	@%p5 bra 	BB1_5;

	neg.f32 	%f9, %f1;
	sqrt.rn.f32 	%f10, %f9;
	neg.f32 	%f11, %f10;

BB1_5:
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.u32 	%rd11, %r3, 8;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.v2.f32 	[%rd12], {%f11, %f11};

BB1_6:
	ret;
}

	// .globl	generateSpectrumKernel
.visible .entry generateSpectrumKernel(
	.param .u64 generateSpectrumKernel_param_0,
	.param .u64 generateSpectrumKernel_param_1,
	.param .u64 generateSpectrumKernel_param_2,
	.param .u32 generateSpectrumKernel_param_3,
	.param .u32 generateSpectrumKernel_param_4,
	.param .u32 generateSpectrumKernel_param_5,
	.param .f32 generateSpectrumKernel_param_6,
	.param .f32 generateSpectrumKernel_param_7,
	.param .f32 generateSpectrumKernel_param_8
)
{
	.local .align 4 .b8 	__local_depot2[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<56>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<389>;
	.reg .b64 	%rd<68>;


	mov.u64 	%rd67, __local_depot2;
	cvta.local.u64 	%SP, %rd67;
	ld.param.u64 	%rd25, [generateSpectrumKernel_param_0];
	ld.param.u64 	%rd26, [generateSpectrumKernel_param_1];
	ld.param.u64 	%rd27, [generateSpectrumKernel_param_2];
	ld.param.u32 	%r111, [generateSpectrumKernel_param_3];
	ld.param.u32 	%r112, [generateSpectrumKernel_param_4];
	ld.param.u32 	%r113, [generateSpectrumKernel_param_5];
	ld.param.f32 	%f74, [generateSpectrumKernel_param_6];
	ld.param.f32 	%f73, [generateSpectrumKernel_param_7];
	ld.param.f32 	%f75, [generateSpectrumKernel_param_8];
	mov.u32 	%r114, %ntid.x;
	mov.u32 	%r115, %ctaid.x;
	mov.u32 	%r116, %tid.x;
	mad.lo.s32 	%r117, %r114, %r115, %r116;
	mov.u32 	%r118, %ntid.y;
	mov.u32 	%r119, %ctaid.y;
	mov.u32 	%r120, %tid.y;
	mad.lo.s32 	%r121, %r118, %r119, %r120;
	neg.s32 	%r122, %r112;
	cvt.rn.f32.s32	%f76, %r122;
	cvt.rn.f32.u32	%f77, %r117;
	fma.rn.f32 	%f78, %f76, 0f3F000000, %f77;
	mov.f32 	%f79, 0f40C90FDB;
	div.rn.f32 	%f80, %f79, %f75;
	mul.f32 	%f81, %f78, %f80;
	neg.s32 	%r123, %r113;
	cvt.rn.f32.s32	%f82, %r123;
	cvt.rn.f32.u32	%f83, %r121;
	fma.rn.f32 	%f84, %f82, 0f3F000000, %f83;
	mul.f32 	%f85, %f80, %f84;
	mul.f32 	%f86, %f85, %f85;
	fma.rn.f32 	%f87, %f81, %f81, %f86;
	sqrt.rn.f32 	%f88, %f87;
	mul.f32 	%f89, %f88, 0f411CF5C3;
	sqrt.rn.f32 	%f90, %f89;
	mul.f32 	%f1, %f90, %f74;
	abs.f32 	%f2, %f1;
	setp.neu.f32	%p1, %f2, 0f7F800000;
	mov.f32 	%f238, %f1;
	@%p1 bra 	BB2_2;

	mov.f32 	%f91, 0f00000000;
	mul.rn.f32 	%f3, %f1, %f91;
	mov.f32 	%f238, %f3;

BB2_2:
	mov.f32 	%f4, %f238;
	mul.f32 	%f92, %f4, 0f3F22F983;
	cvt.rni.s32.f32	%r364, %f92;
	cvt.rn.f32.s32	%f93, %r364;
	neg.f32 	%f94, %f93;
	mov.f32 	%f95, 0f3FC90FDA;
	fma.rn.f32 	%f96, %f94, %f95, %f4;
	mov.f32 	%f97, 0f33A22168;
	fma.rn.f32 	%f98, %f94, %f97, %f96;
	mov.f32 	%f99, 0f27C234C5;
	fma.rn.f32 	%f232, %f94, %f99, %f98;
	abs.f32 	%f100, %f4;
	setp.leu.f32	%p2, %f100, 0f47CE4780;
	@%p2 bra 	BB2_10;

	mov.b32 	 %r2, %f4;
	shl.b32 	%r126, %r2, 8;
	or.b32  	%r3, %r126, -2147483648;
	add.u64 	%rd29, %SP, 0;
	cvta.to.local.u64 	%rd60, %rd29;
	mov.u32 	%r358, 0;
	mov.u64 	%rd59, __cudart_i2opi_f;
	mov.u32 	%r357, -6;

BB2_4:
	.pragma "nounroll";
	ld.const.u32 	%r129, [%rd59];
	// inline asm
	{
	mad.lo.cc.u32   %r127, %r129, %r3, %r358;
	madc.hi.u32     %r358, %r129, %r3,  0;
	}
	// inline asm
	st.local.u32 	[%rd60], %r127;
	add.s64 	%rd60, %rd60, 4;
	add.s64 	%rd59, %rd59, 4;
	add.s32 	%r357, %r357, 1;
	setp.ne.s32	%p3, %r357, 0;
	@%p3 bra 	BB2_4;

	bfe.u32 	%r132, %r2, 23, 8;
	add.s32 	%r133, %r132, -128;
	shr.u32 	%r134, %r133, 5;
	and.b32  	%r8, %r2, -2147483648;
	cvta.to.local.u64 	%rd31, %rd29;
	st.local.u32 	[%rd31+24], %r358;
	bfe.u32 	%r9, %r2, 23, 5;
	mov.u32 	%r135, 6;
	sub.s32 	%r136, %r135, %r134;
	mul.wide.s32 	%rd32, %r136, 4;
	add.s64 	%rd6, %rd31, %rd32;
	ld.local.u32 	%r359, [%rd6];
	ld.local.u32 	%r360, [%rd6+-4];
	setp.eq.s32	%p4, %r9, 0;
	@%p4 bra 	BB2_7;

	mov.u32 	%r137, 32;
	sub.s32 	%r138, %r137, %r9;
	shr.u32 	%r139, %r360, %r138;
	shl.b32 	%r140, %r359, %r9;
	add.s32 	%r359, %r139, %r140;
	ld.local.u32 	%r141, [%rd6+-8];
	shr.u32 	%r142, %r141, %r138;
	shl.b32 	%r143, %r360, %r9;
	add.s32 	%r360, %r142, %r143;

BB2_7:
	shr.u32 	%r144, %r360, 30;
	shl.b32 	%r145, %r359, 2;
	add.s32 	%r361, %r144, %r145;
	shl.b32 	%r17, %r360, 2;
	shr.u32 	%r146, %r361, 31;
	shr.u32 	%r147, %r359, 30;
	add.s32 	%r18, %r146, %r147;
	setp.eq.s32	%p5, %r146, 0;
	mov.u32 	%r362, %r8;
	mov.u32 	%r363, %r17;
	@%p5 bra 	BB2_9;

	not.b32 	%r148, %r361;
	neg.s32 	%r19, %r17;
	setp.eq.s32	%p6, %r17, 0;
	selp.u32	%r149, 1, 0, %p6;
	add.s32 	%r361, %r149, %r148;
	xor.b32  	%r21, %r8, -2147483648;
	mov.u32 	%r362, %r21;
	mov.u32 	%r363, %r19;

BB2_9:
	mov.u32 	%r23, %r362;
	neg.s32 	%r150, %r18;
	setp.ne.s32	%p7, %r8, 0;
	selp.b32	%r364, %r150, %r18, %p7;
	clz.b32 	%r151, %r361;
	setp.ne.s32	%p8, %r151, 0;
	shl.b32 	%r152, %r361, %r151;
	mov.u32 	%r153, 32;
	sub.s32 	%r154, %r153, %r151;
	shr.u32 	%r155, %r363, %r154;
	add.s32 	%r156, %r155, %r152;
	selp.b32	%r157, %r156, %r361, %p8;
	mul.lo.s32 	%r158, %r157, -921707870;
	mov.u32 	%r159, -921707870;
	mul.hi.u32 	%r160, %r157, %r159;
	setp.gt.s32	%p9, %r160, 0;
	shl.b32 	%r161, %r160, 1;
	shr.u32 	%r162, %r158, 31;
	add.s32 	%r163, %r162, %r161;
	selp.b32	%r164, %r163, %r160, %p9;
	selp.b32	%r165, -1, 0, %p9;
	mov.u32 	%r166, 126;
	sub.s32 	%r167, %r166, %r151;
	add.s32 	%r168, %r167, %r165;
	shl.b32 	%r169, %r168, 23;
	add.s32 	%r170, %r164, 1;
	shr.u32 	%r171, %r170, 7;
	add.s32 	%r172, %r171, 1;
	shr.u32 	%r173, %r172, 1;
	add.s32 	%r174, %r173, %r169;
	or.b32  	%r175, %r174, %r23;
	mov.b32 	 %f232, %r175;

BB2_10:
	mul.rn.f32 	%f8, %f232, %f232;
	add.s32 	%r27, %r364, 1;
	and.b32  	%r28, %r27, 1;
	setp.eq.s32	%p10, %r28, 0;
	@%p10 bra 	BB2_12;

	mov.f32 	%f101, 0fBAB6061A;
	mov.f32 	%f102, 0f37CCF5CE;
	fma.rn.f32 	%f233, %f102, %f8, %f101;
	bra.uni 	BB2_13;

BB2_12:
	mov.f32 	%f103, 0f3C08839E;
	mov.f32 	%f104, 0fB94CA1F9;
	fma.rn.f32 	%f233, %f104, %f8, %f103;

BB2_13:
	@%p10 bra 	BB2_15;

	mov.f32 	%f105, 0f3D2AAAA5;
	fma.rn.f32 	%f106, %f233, %f8, %f105;
	mov.f32 	%f107, 0fBF000000;
	fma.rn.f32 	%f234, %f106, %f8, %f107;
	bra.uni 	BB2_16;

BB2_15:
	mov.f32 	%f108, 0fBE2AAAA3;
	fma.rn.f32 	%f109, %f233, %f8, %f108;
	mov.f32 	%f110, 0f00000000;
	fma.rn.f32 	%f234, %f109, %f8, %f110;

BB2_16:
	fma.rn.f32 	%f235, %f234, %f232, %f232;
	@%p10 bra 	BB2_18;

	mov.f32 	%f111, 0f3F800000;
	fma.rn.f32 	%f235, %f234, %f8, %f111;

BB2_18:
	and.b32  	%r176, %r27, 2;
	setp.eq.s32	%p13, %r176, 0;
	@%p13 bra 	BB2_20;

	mov.f32 	%f112, 0f00000000;
	mov.f32 	%f113, 0fBF800000;
	fma.rn.f32 	%f235, %f235, %f113, %f112;

BB2_20:
	mov.f32 	%f237, %f1;
	@%p1 bra 	BB2_22;

	mov.f32 	%f114, 0f00000000;
	mul.rn.f32 	%f237, %f1, %f114;

BB2_22:
	mul.f32 	%f115, %f237, 0f3F22F983;
	cvt.rni.s32.f32	%r372, %f115;
	cvt.rn.f32.s32	%f116, %r372;
	neg.f32 	%f117, %f116;
	fma.rn.f32 	%f119, %f117, %f95, %f237;
	fma.rn.f32 	%f121, %f117, %f97, %f119;
	fma.rn.f32 	%f239, %f117, %f99, %f121;
	abs.f32 	%f123, %f237;
	setp.leu.f32	%p15, %f123, 0f47CE4780;
	@%p15 bra 	BB2_30;

	mov.b32 	 %r30, %f237;
	shl.b32 	%r179, %r30, 8;
	or.b32  	%r31, %r179, -2147483648;
	add.u64 	%rd34, %SP, 0;
	cvta.to.local.u64 	%rd62, %rd34;
	mov.u32 	%r366, 0;
	mov.u64 	%rd61, __cudart_i2opi_f;
	mov.u32 	%r365, -6;

BB2_24:
	.pragma "nounroll";
	ld.const.u32 	%r182, [%rd61];
	// inline asm
	{
	mad.lo.cc.u32   %r180, %r182, %r31, %r366;
	madc.hi.u32     %r366, %r182, %r31,  0;
	}
	// inline asm
	st.local.u32 	[%rd62], %r180;
	add.s64 	%rd62, %rd62, 4;
	add.s64 	%rd61, %rd61, 4;
	add.s32 	%r365, %r365, 1;
	setp.ne.s32	%p16, %r365, 0;
	@%p16 bra 	BB2_24;

	bfe.u32 	%r185, %r30, 23, 8;
	add.s32 	%r186, %r185, -128;
	shr.u32 	%r187, %r186, 5;
	and.b32  	%r36, %r30, -2147483648;
	cvta.to.local.u64 	%rd36, %rd34;
	st.local.u32 	[%rd36+24], %r366;
	bfe.u32 	%r37, %r30, 23, 5;
	mov.u32 	%r188, 6;
	sub.s32 	%r189, %r188, %r187;
	mul.wide.s32 	%rd37, %r189, 4;
	add.s64 	%rd12, %rd36, %rd37;
	ld.local.u32 	%r367, [%rd12];
	ld.local.u32 	%r368, [%rd12+-4];
	setp.eq.s32	%p17, %r37, 0;
	@%p17 bra 	BB2_27;

	mov.u32 	%r190, 32;
	sub.s32 	%r191, %r190, %r37;
	shr.u32 	%r192, %r368, %r191;
	shl.b32 	%r193, %r367, %r37;
	add.s32 	%r367, %r192, %r193;
	ld.local.u32 	%r194, [%rd12+-8];
	shr.u32 	%r195, %r194, %r191;
	shl.b32 	%r196, %r368, %r37;
	add.s32 	%r368, %r195, %r196;

BB2_27:
	shr.u32 	%r197, %r368, 30;
	shl.b32 	%r198, %r367, 2;
	add.s32 	%r369, %r197, %r198;
	shl.b32 	%r45, %r368, 2;
	shr.u32 	%r199, %r369, 31;
	shr.u32 	%r200, %r367, 30;
	add.s32 	%r46, %r199, %r200;
	setp.eq.s32	%p18, %r199, 0;
	mov.u32 	%r370, %r36;
	mov.u32 	%r371, %r45;
	@%p18 bra 	BB2_29;

	not.b32 	%r201, %r369;
	neg.s32 	%r47, %r45;
	setp.eq.s32	%p19, %r45, 0;
	selp.u32	%r202, 1, 0, %p19;
	add.s32 	%r369, %r202, %r201;
	xor.b32  	%r49, %r36, -2147483648;
	mov.u32 	%r370, %r49;
	mov.u32 	%r371, %r47;

BB2_29:
	mov.u32 	%r51, %r370;
	neg.s32 	%r203, %r46;
	setp.ne.s32	%p20, %r36, 0;
	selp.b32	%r372, %r203, %r46, %p20;
	clz.b32 	%r204, %r369;
	setp.ne.s32	%p21, %r204, 0;
	shl.b32 	%r205, %r369, %r204;
	mov.u32 	%r206, 32;
	sub.s32 	%r207, %r206, %r204;
	shr.u32 	%r208, %r371, %r207;
	add.s32 	%r209, %r208, %r205;
	selp.b32	%r210, %r209, %r369, %p21;
	mul.lo.s32 	%r211, %r210, -921707870;
	mov.u32 	%r212, -921707870;
	mul.hi.u32 	%r213, %r210, %r212;
	setp.gt.s32	%p22, %r213, 0;
	shl.b32 	%r214, %r213, 1;
	shr.u32 	%r215, %r211, 31;
	add.s32 	%r216, %r215, %r214;
	selp.b32	%r217, %r216, %r213, %p22;
	selp.b32	%r218, -1, 0, %p22;
	mov.u32 	%r219, 126;
	sub.s32 	%r220, %r219, %r204;
	add.s32 	%r221, %r220, %r218;
	shl.b32 	%r222, %r221, 23;
	add.s32 	%r223, %r217, 1;
	shr.u32 	%r224, %r223, 7;
	add.s32 	%r225, %r224, 1;
	shr.u32 	%r226, %r225, 1;
	add.s32 	%r227, %r226, %r222;
	or.b32  	%r228, %r227, %r51;
	mov.b32 	 %f239, %r228;

BB2_30:
	mul.rn.f32 	%f25, %f239, %f239;
	and.b32  	%r55, %r372, 1;
	setp.eq.s32	%p23, %r55, 0;
	@%p23 bra 	BB2_32;

	mov.f32 	%f124, 0fBAB6061A;
	mov.f32 	%f125, 0f37CCF5CE;
	fma.rn.f32 	%f240, %f125, %f25, %f124;
	bra.uni 	BB2_33;

BB2_32:
	mov.f32 	%f126, 0f3C08839E;
	mov.f32 	%f127, 0fB94CA1F9;
	fma.rn.f32 	%f240, %f127, %f25, %f126;

BB2_33:
	@%p23 bra 	BB2_35;

	mov.f32 	%f128, 0f3D2AAAA5;
	fma.rn.f32 	%f129, %f240, %f25, %f128;
	mov.f32 	%f130, 0fBF000000;
	fma.rn.f32 	%f241, %f129, %f25, %f130;
	bra.uni 	BB2_36;

BB2_35:
	mov.f32 	%f131, 0fBE2AAAA3;
	fma.rn.f32 	%f132, %f240, %f25, %f131;
	mov.f32 	%f133, 0f00000000;
	fma.rn.f32 	%f241, %f132, %f25, %f133;

BB2_36:
	fma.rn.f32 	%f242, %f241, %f239, %f239;
	@%p23 bra 	BB2_38;

	mov.f32 	%f134, 0f3F800000;
	fma.rn.f32 	%f242, %f241, %f25, %f134;

BB2_38:
	and.b32  	%r229, %r372, 2;
	setp.eq.s32	%p26, %r229, 0;
	@%p26 bra 	BB2_40;

	mov.f32 	%f135, 0f00000000;
	mov.f32 	%f136, 0fBF800000;
	fma.rn.f32 	%f242, %f242, %f136, %f135;

BB2_40:
	neg.f32 	%f37, %f1;
	abs.f32 	%f38, %f37;
	setp.neu.f32	%p27, %f38, 0f7F800000;
	mov.f32 	%f249, %f37;
	@%p27 bra 	BB2_42;

	mov.f32 	%f137, 0f00000000;
	mul.rn.f32 	%f39, %f37, %f137;
	mov.f32 	%f249, %f39;

BB2_42:
	mov.f32 	%f40, %f249;
	mul.f32 	%f138, %f40, 0f3F22F983;
	cvt.rni.s32.f32	%r380, %f138;
	cvt.rn.f32.s32	%f139, %r380;
	neg.f32 	%f140, %f139;
	fma.rn.f32 	%f142, %f140, %f95, %f40;
	fma.rn.f32 	%f144, %f140, %f97, %f142;
	fma.rn.f32 	%f243, %f140, %f99, %f144;
	abs.f32 	%f146, %f40;
	setp.leu.f32	%p28, %f146, 0f47CE4780;
	@%p28 bra 	BB2_50;

	mov.b32 	 %r57, %f40;
	shl.b32 	%r232, %r57, 8;
	or.b32  	%r58, %r232, -2147483648;
	add.u64 	%rd39, %SP, 0;
	cvta.to.local.u64 	%rd64, %rd39;
	mov.u32 	%r374, 0;
	mov.u64 	%rd63, __cudart_i2opi_f;
	mov.u32 	%r373, -6;

BB2_44:
	.pragma "nounroll";
	ld.const.u32 	%r235, [%rd63];
	// inline asm
	{
	mad.lo.cc.u32   %r233, %r235, %r58, %r374;
	madc.hi.u32     %r374, %r235, %r58,  0;
	}
	// inline asm
	st.local.u32 	[%rd64], %r233;
	add.s64 	%rd64, %rd64, 4;
	add.s64 	%rd63, %rd63, 4;
	add.s32 	%r373, %r373, 1;
	setp.ne.s32	%p29, %r373, 0;
	@%p29 bra 	BB2_44;

	bfe.u32 	%r238, %r57, 23, 8;
	add.s32 	%r239, %r238, -128;
	shr.u32 	%r240, %r239, 5;
	and.b32  	%r63, %r57, -2147483648;
	cvta.to.local.u64 	%rd41, %rd39;
	st.local.u32 	[%rd41+24], %r374;
	bfe.u32 	%r64, %r57, 23, 5;
	mov.u32 	%r241, 6;
	sub.s32 	%r242, %r241, %r240;
	mul.wide.s32 	%rd42, %r242, 4;
	add.s64 	%rd18, %rd41, %rd42;
	ld.local.u32 	%r375, [%rd18];
	ld.local.u32 	%r376, [%rd18+-4];
	setp.eq.s32	%p30, %r64, 0;
	@%p30 bra 	BB2_47;

	mov.u32 	%r243, 32;
	sub.s32 	%r244, %r243, %r64;
	shr.u32 	%r245, %r376, %r244;
	shl.b32 	%r246, %r375, %r64;
	add.s32 	%r375, %r245, %r246;
	ld.local.u32 	%r247, [%rd18+-8];
	shr.u32 	%r248, %r247, %r244;
	shl.b32 	%r249, %r376, %r64;
	add.s32 	%r376, %r248, %r249;

BB2_47:
	shr.u32 	%r250, %r376, 30;
	shl.b32 	%r251, %r375, 2;
	add.s32 	%r377, %r250, %r251;
	shl.b32 	%r72, %r376, 2;
	shr.u32 	%r252, %r377, 31;
	shr.u32 	%r253, %r375, 30;
	add.s32 	%r73, %r252, %r253;
	setp.eq.s32	%p31, %r252, 0;
	mov.u32 	%r378, %r63;
	mov.u32 	%r379, %r72;
	@%p31 bra 	BB2_49;

	not.b32 	%r254, %r377;
	neg.s32 	%r74, %r72;
	setp.eq.s32	%p32, %r72, 0;
	selp.u32	%r255, 1, 0, %p32;
	add.s32 	%r377, %r255, %r254;
	xor.b32  	%r76, %r63, -2147483648;
	mov.u32 	%r378, %r76;
	mov.u32 	%r379, %r74;

BB2_49:
	mov.u32 	%r78, %r378;
	neg.s32 	%r256, %r73;
	setp.ne.s32	%p33, %r63, 0;
	selp.b32	%r380, %r256, %r73, %p33;
	clz.b32 	%r257, %r377;
	setp.ne.s32	%p34, %r257, 0;
	shl.b32 	%r258, %r377, %r257;
	mov.u32 	%r259, 32;
	sub.s32 	%r260, %r259, %r257;
	shr.u32 	%r261, %r379, %r260;
	add.s32 	%r262, %r261, %r258;
	selp.b32	%r263, %r262, %r377, %p34;
	mul.lo.s32 	%r264, %r263, -921707870;
	mov.u32 	%r265, -921707870;
	mul.hi.u32 	%r266, %r263, %r265;
	setp.gt.s32	%p35, %r266, 0;
	shl.b32 	%r267, %r266, 1;
	shr.u32 	%r268, %r264, 31;
	add.s32 	%r269, %r268, %r267;
	selp.b32	%r270, %r269, %r266, %p35;
	selp.b32	%r271, -1, 0, %p35;
	mov.u32 	%r272, 126;
	sub.s32 	%r273, %r272, %r257;
	add.s32 	%r274, %r273, %r271;
	shl.b32 	%r275, %r274, 23;
	add.s32 	%r276, %r270, 1;
	shr.u32 	%r277, %r276, 7;
	add.s32 	%r278, %r277, 1;
	shr.u32 	%r279, %r278, 1;
	add.s32 	%r280, %r279, %r275;
	or.b32  	%r281, %r280, %r78;
	mov.b32 	 %f243, %r281;

BB2_50:
	mul.rn.f32 	%f44, %f243, %f243;
	add.s32 	%r82, %r380, 1;
	and.b32  	%r83, %r82, 1;
	setp.eq.s32	%p36, %r83, 0;
	@%p36 bra 	BB2_52;

	mov.f32 	%f147, 0fBAB6061A;
	mov.f32 	%f148, 0f37CCF5CE;
	fma.rn.f32 	%f244, %f148, %f44, %f147;
	bra.uni 	BB2_53;

BB2_52:
	mov.f32 	%f149, 0f3C08839E;
	mov.f32 	%f150, 0fB94CA1F9;
	fma.rn.f32 	%f244, %f150, %f44, %f149;

BB2_53:
	@%p36 bra 	BB2_55;

	mov.f32 	%f151, 0f3D2AAAA5;
	fma.rn.f32 	%f152, %f244, %f44, %f151;
	mov.f32 	%f153, 0fBF000000;
	fma.rn.f32 	%f245, %f152, %f44, %f153;
	bra.uni 	BB2_56;

BB2_55:
	mov.f32 	%f154, 0fBE2AAAA3;
	fma.rn.f32 	%f155, %f244, %f44, %f154;
	mov.f32 	%f156, 0f00000000;
	fma.rn.f32 	%f245, %f155, %f44, %f156;

BB2_56:
	fma.rn.f32 	%f246, %f245, %f243, %f243;
	@%p36 bra 	BB2_58;

	mov.f32 	%f157, 0f3F800000;
	fma.rn.f32 	%f246, %f245, %f44, %f157;

BB2_58:
	and.b32  	%r282, %r82, 2;
	setp.eq.s32	%p39, %r282, 0;
	@%p39 bra 	BB2_60;

	mov.f32 	%f158, 0f00000000;
	mov.f32 	%f159, 0fBF800000;
	fma.rn.f32 	%f246, %f246, %f159, %f158;

BB2_60:
	mov.f32 	%f248, %f37;
	@%p27 bra 	BB2_62;

	mov.f32 	%f160, 0f00000000;
	mul.rn.f32 	%f248, %f37, %f160;

BB2_62:
	mul.f32 	%f161, %f248, 0f3F22F983;
	cvt.rni.s32.f32	%r388, %f161;
	cvt.rn.f32.s32	%f162, %r388;
	neg.f32 	%f163, %f162;
	fma.rn.f32 	%f165, %f163, %f95, %f248;
	fma.rn.f32 	%f167, %f163, %f97, %f165;
	fma.rn.f32 	%f250, %f163, %f99, %f167;
	abs.f32 	%f169, %f248;
	setp.leu.f32	%p41, %f169, 0f47CE4780;
	@%p41 bra 	BB2_70;

	mov.b32 	 %r85, %f248;
	shl.b32 	%r285, %r85, 8;
	or.b32  	%r86, %r285, -2147483648;
	add.u64 	%rd44, %SP, 0;
	cvta.to.local.u64 	%rd66, %rd44;
	mov.u32 	%r382, 0;
	mov.u64 	%rd65, __cudart_i2opi_f;
	mov.u32 	%r381, -6;

BB2_64:
	.pragma "nounroll";
	ld.const.u32 	%r288, [%rd65];
	// inline asm
	{
	mad.lo.cc.u32   %r286, %r288, %r86, %r382;
	madc.hi.u32     %r382, %r288, %r86,  0;
	}
	// inline asm
	st.local.u32 	[%rd66], %r286;
	add.s64 	%rd66, %rd66, 4;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r381, %r381, 1;
	setp.ne.s32	%p42, %r381, 0;
	@%p42 bra 	BB2_64;

	bfe.u32 	%r291, %r85, 23, 8;
	add.s32 	%r292, %r291, -128;
	shr.u32 	%r293, %r292, 5;
	and.b32  	%r91, %r85, -2147483648;
	cvta.to.local.u64 	%rd46, %rd44;
	st.local.u32 	[%rd46+24], %r382;
	bfe.u32 	%r92, %r85, 23, 5;
	mov.u32 	%r294, 6;
	sub.s32 	%r295, %r294, %r293;
	mul.wide.s32 	%rd47, %r295, 4;
	add.s64 	%rd24, %rd46, %rd47;
	ld.local.u32 	%r383, [%rd24];
	ld.local.u32 	%r384, [%rd24+-4];
	setp.eq.s32	%p43, %r92, 0;
	@%p43 bra 	BB2_67;

	mov.u32 	%r296, 32;
	sub.s32 	%r297, %r296, %r92;
	shr.u32 	%r298, %r384, %r297;
	shl.b32 	%r299, %r383, %r92;
	add.s32 	%r383, %r298, %r299;
	ld.local.u32 	%r300, [%rd24+-8];
	shr.u32 	%r301, %r300, %r297;
	shl.b32 	%r302, %r384, %r92;
	add.s32 	%r384, %r301, %r302;

BB2_67:
	shr.u32 	%r303, %r384, 30;
	shl.b32 	%r304, %r383, 2;
	add.s32 	%r385, %r303, %r304;
	shl.b32 	%r100, %r384, 2;
	shr.u32 	%r305, %r385, 31;
	shr.u32 	%r306, %r383, 30;
	add.s32 	%r101, %r305, %r306;
	setp.eq.s32	%p44, %r305, 0;
	mov.u32 	%r386, %r91;
	mov.u32 	%r387, %r100;
	@%p44 bra 	BB2_69;

	not.b32 	%r307, %r385;
	neg.s32 	%r102, %r100;
	setp.eq.s32	%p45, %r100, 0;
	selp.u32	%r308, 1, 0, %p45;
	add.s32 	%r385, %r308, %r307;
	xor.b32  	%r104, %r91, -2147483648;
	mov.u32 	%r386, %r104;
	mov.u32 	%r387, %r102;

BB2_69:
	mov.u32 	%r106, %r386;
	neg.s32 	%r309, %r101;
	setp.ne.s32	%p46, %r91, 0;
	selp.b32	%r388, %r309, %r101, %p46;
	clz.b32 	%r310, %r385;
	setp.ne.s32	%p47, %r310, 0;
	shl.b32 	%r311, %r385, %r310;
	mov.u32 	%r312, 32;
	sub.s32 	%r313, %r312, %r310;
	shr.u32 	%r314, %r387, %r313;
	add.s32 	%r315, %r314, %r311;
	selp.b32	%r316, %r315, %r385, %p47;
	mul.lo.s32 	%r317, %r316, -921707870;
	mov.u32 	%r318, -921707870;
	mul.hi.u32 	%r319, %r316, %r318;
	setp.gt.s32	%p48, %r319, 0;
	shl.b32 	%r320, %r319, 1;
	shr.u32 	%r321, %r317, 31;
	add.s32 	%r322, %r321, %r320;
	selp.b32	%r323, %r322, %r319, %p48;
	selp.b32	%r324, -1, 0, %p48;
	mov.u32 	%r325, 126;
	sub.s32 	%r326, %r325, %r310;
	add.s32 	%r327, %r326, %r324;
	shl.b32 	%r328, %r327, 23;
	add.s32 	%r329, %r323, 1;
	shr.u32 	%r330, %r329, 7;
	add.s32 	%r331, %r330, 1;
	shr.u32 	%r332, %r331, 1;
	add.s32 	%r333, %r332, %r328;
	or.b32  	%r334, %r333, %r106;
	mov.b32 	 %f250, %r334;

BB2_70:
	mul.rn.f32 	%f61, %f250, %f250;
	and.b32  	%r110, %r388, 1;
	setp.eq.s32	%p49, %r110, 0;
	@%p49 bra 	BB2_72;

	mov.f32 	%f170, 0fBAB6061A;
	mov.f32 	%f171, 0f37CCF5CE;
	fma.rn.f32 	%f251, %f171, %f61, %f170;
	bra.uni 	BB2_73;

BB2_72:
	mov.f32 	%f172, 0f3C08839E;
	mov.f32 	%f173, 0fB94CA1F9;
	fma.rn.f32 	%f251, %f173, %f61, %f172;

BB2_73:
	@%p49 bra 	BB2_75;

	mov.f32 	%f174, 0f3D2AAAA5;
	fma.rn.f32 	%f175, %f251, %f61, %f174;
	mov.f32 	%f176, 0fBF000000;
	fma.rn.f32 	%f252, %f175, %f61, %f176;
	bra.uni 	BB2_76;

BB2_75:
	mov.f32 	%f177, 0fBE2AAAA3;
	fma.rn.f32 	%f178, %f251, %f61, %f177;
	mov.f32 	%f179, 0f00000000;
	fma.rn.f32 	%f252, %f178, %f61, %f179;

BB2_76:
	fma.rn.f32 	%f253, %f252, %f250, %f250;
	@%p49 bra 	BB2_78;

	mov.f32 	%f180, 0f3F800000;
	fma.rn.f32 	%f253, %f252, %f61, %f180;

BB2_78:
	and.b32  	%r335, %r388, 2;
	setp.eq.s32	%p52, %r335, 0;
	@%p52 bra 	BB2_80;

	mov.f32 	%f181, 0f00000000;
	mov.f32 	%f182, 0fBF800000;
	fma.rn.f32 	%f253, %f253, %f182, %f181;

BB2_80:
	setp.lt.u32	%p53, %r117, %r112;
	setp.lt.u32	%p54, %r121, %r113;
	and.pred  	%p55, %p53, %p54;
	@!%p55 bra 	BB2_82;
	bra.uni 	BB2_81;

BB2_81:
	cvta.to.global.u64 	%rd48, %rd26;
	mad.lo.s32 	%r352, %r121, %r111, %r117;
	cvta.to.global.u64 	%rd49, %rd25;
	mul.wide.u32 	%rd50, %r352, 8;
	add.s64 	%rd51, %rd49, %rd50;
	sub.s32 	%r353, %r112, %r117;
	sub.s32 	%r354, %r113, %r121;
	mad.lo.s32 	%r355, %r354, %r111, %r353;
	mul.wide.u32 	%rd52, %r355, 8;
	add.s64 	%rd53, %rd49, %rd52;
	cvta.to.global.u64 	%rd54, %rd27;
	add.s64 	%rd55, %rd54, %rd50;
	add.s64 	%rd56, %rd54, %rd52;
	ld.global.v2.f32 	{%f183, %f184}, [%rd51];
	mul.f32 	%f187, %f235, %f183;
	mul.f32 	%f188, %f242, %f184;
	sub.f32 	%f189, %f187, %f188;
	mul.f32 	%f190, %f235, %f184;
	fma.rn.f32 	%f191, %f242, %f183, %f190;
	ld.global.v2.f32 	{%f192, %f193}, [%rd53];
	mul.f32 	%f196, %f253, %f193;
	fma.rn.f32 	%f197, %f246, %f192, %f196;
	mul.f32 	%f198, %f253, %f192;
	mul.f32 	%f199, %f246, %f193;
	sub.f32 	%f200, %f198, %f199;
	add.f32 	%f201, %f189, %f197;
	add.f32 	%f202, %f191, %f200;
	ld.global.v2.f32 	{%f203, %f204}, [%rd55];
	mul.f32 	%f207, %f235, %f203;
	mul.f32 	%f208, %f242, %f204;
	sub.f32 	%f209, %f207, %f208;
	mul.f32 	%f210, %f235, %f204;
	fma.rn.f32 	%f211, %f242, %f203, %f210;
	ld.global.v2.f32 	{%f212, %f213}, [%rd56];
	mul.f32 	%f216, %f253, %f213;
	fma.rn.f32 	%f217, %f246, %f212, %f216;
	mul.f32 	%f218, %f253, %f212;
	mul.f32 	%f219, %f246, %f213;
	sub.f32 	%f220, %f218, %f219;
	add.f32 	%f221, %f209, %f217;
	add.f32 	%f222, %f211, %f220;
	mul.f32 	%f223, %f201, %f221;
	mul.f32 	%f224, %f202, %f222;
	sub.f32 	%f225, %f223, %f224;
	mul.f32 	%f226, %f202, %f221;
	fma.rn.f32 	%f227, %f201, %f222, %f226;
	mad.lo.s32 	%r356, %r121, %r112, %r117;
	sub.f32 	%f228, %f225, %f201;
	sub.f32 	%f229, %f227, %f202;
	mul.wide.u32 	%rd57, %r356, 8;
	add.s64 	%rd58, %rd48, %rd57;
	fma.rn.f32 	%f230, %f228, %f73, %f201;
	fma.rn.f32 	%f231, %f229, %f73, %f202;
	st.global.v2.f32 	[%rd58], {%f230, %f231};

BB2_82:
	ret;
}

	// .globl	generateSpectrumKernel2
.visible .entry generateSpectrumKernel2(
	.param .u64 generateSpectrumKernel2_param_0,
	.param .u64 generateSpectrumKernel2_param_1,
	.param .u64 generateSpectrumKernel2_param_2,
	.param .u32 generateSpectrumKernel2_param_3,
	.param .u32 generateSpectrumKernel2_param_4,
	.param .u32 generateSpectrumKernel2_param_5,
	.param .f32 generateSpectrumKernel2_param_6,
	.param .f32 generateSpectrumKernel2_param_7,
	.param .f32 generateSpectrumKernel2_param_8
)
{
	.local .align 4 .b8 	__local_depot3[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<56>;
	.reg .f32 	%f<251>;
	.reg .b32 	%r<389>;
	.reg .b64 	%rd<68>;


	mov.u64 	%rd67, __local_depot3;
	cvta.local.u64 	%SP, %rd67;
	ld.param.u64 	%rd25, [generateSpectrumKernel2_param_0];
	ld.param.u64 	%rd26, [generateSpectrumKernel2_param_1];
	ld.param.u64 	%rd27, [generateSpectrumKernel2_param_2];
	ld.param.u32 	%r111, [generateSpectrumKernel2_param_3];
	ld.param.u32 	%r112, [generateSpectrumKernel2_param_4];
	ld.param.u32 	%r113, [generateSpectrumKernel2_param_5];
	ld.param.f32 	%f74, [generateSpectrumKernel2_param_6];
	ld.param.f32 	%f73, [generateSpectrumKernel2_param_7];
	ld.param.f32 	%f75, [generateSpectrumKernel2_param_8];
	mov.u32 	%r114, %ntid.x;
	mov.u32 	%r115, %ctaid.x;
	mov.u32 	%r116, %tid.x;
	mad.lo.s32 	%r117, %r114, %r115, %r116;
	mov.u32 	%r118, %ntid.y;
	mov.u32 	%r119, %ctaid.y;
	mov.u32 	%r120, %tid.y;
	mad.lo.s32 	%r121, %r118, %r119, %r120;
	neg.s32 	%r122, %r112;
	cvt.rn.f32.s32	%f76, %r122;
	cvt.rn.f32.u32	%f77, %r117;
	fma.rn.f32 	%f78, %f76, 0f3F000000, %f77;
	mov.f32 	%f79, 0f40C90FDB;
	div.rn.f32 	%f80, %f79, %f75;
	mul.f32 	%f81, %f78, %f80;
	neg.s32 	%r123, %r113;
	cvt.rn.f32.s32	%f82, %r123;
	cvt.rn.f32.u32	%f83, %r121;
	fma.rn.f32 	%f84, %f82, 0f3F000000, %f83;
	mul.f32 	%f85, %f80, %f84;
	mul.f32 	%f86, %f85, %f85;
	fma.rn.f32 	%f87, %f81, %f81, %f86;
	sqrt.rn.f32 	%f88, %f87;
	mul.f32 	%f89, %f88, 0f411CF5C3;
	sqrt.rn.f32 	%f90, %f89;
	mul.f32 	%f1, %f90, %f74;
	abs.f32 	%f2, %f1;
	setp.neu.f32	%p1, %f2, 0f7F800000;
	mov.f32 	%f235, %f1;
	@%p1 bra 	BB3_2;

	mov.f32 	%f91, 0f00000000;
	mul.rn.f32 	%f3, %f1, %f91;
	mov.f32 	%f235, %f3;

BB3_2:
	mov.f32 	%f4, %f235;
	mul.f32 	%f92, %f4, 0f3F22F983;
	cvt.rni.s32.f32	%r364, %f92;
	cvt.rn.f32.s32	%f93, %r364;
	neg.f32 	%f94, %f93;
	mov.f32 	%f95, 0f3FC90FDA;
	fma.rn.f32 	%f96, %f94, %f95, %f4;
	mov.f32 	%f97, 0f33A22168;
	fma.rn.f32 	%f98, %f94, %f97, %f96;
	mov.f32 	%f99, 0f27C234C5;
	fma.rn.f32 	%f229, %f94, %f99, %f98;
	abs.f32 	%f100, %f4;
	setp.leu.f32	%p2, %f100, 0f47CE4780;
	@%p2 bra 	BB3_10;

	mov.b32 	 %r2, %f4;
	shl.b32 	%r126, %r2, 8;
	or.b32  	%r3, %r126, -2147483648;
	add.u64 	%rd29, %SP, 0;
	cvta.to.local.u64 	%rd60, %rd29;
	mov.u32 	%r358, 0;
	mov.u64 	%rd59, __cudart_i2opi_f;
	mov.u32 	%r357, -6;

BB3_4:
	.pragma "nounroll";
	ld.const.u32 	%r129, [%rd59];
	// inline asm
	{
	mad.lo.cc.u32   %r127, %r129, %r3, %r358;
	madc.hi.u32     %r358, %r129, %r3,  0;
	}
	// inline asm
	st.local.u32 	[%rd60], %r127;
	add.s64 	%rd60, %rd60, 4;
	add.s64 	%rd59, %rd59, 4;
	add.s32 	%r357, %r357, 1;
	setp.ne.s32	%p3, %r357, 0;
	@%p3 bra 	BB3_4;

	bfe.u32 	%r132, %r2, 23, 8;
	add.s32 	%r133, %r132, -128;
	shr.u32 	%r134, %r133, 5;
	and.b32  	%r8, %r2, -2147483648;
	cvta.to.local.u64 	%rd31, %rd29;
	st.local.u32 	[%rd31+24], %r358;
	bfe.u32 	%r9, %r2, 23, 5;
	mov.u32 	%r135, 6;
	sub.s32 	%r136, %r135, %r134;
	mul.wide.s32 	%rd32, %r136, 4;
	add.s64 	%rd6, %rd31, %rd32;
	ld.local.u32 	%r359, [%rd6];
	ld.local.u32 	%r360, [%rd6+-4];
	setp.eq.s32	%p4, %r9, 0;
	@%p4 bra 	BB3_7;

	mov.u32 	%r137, 32;
	sub.s32 	%r138, %r137, %r9;
	shr.u32 	%r139, %r360, %r138;
	shl.b32 	%r140, %r359, %r9;
	add.s32 	%r359, %r139, %r140;
	ld.local.u32 	%r141, [%rd6+-8];
	shr.u32 	%r142, %r141, %r138;
	shl.b32 	%r143, %r360, %r9;
	add.s32 	%r360, %r142, %r143;

BB3_7:
	shr.u32 	%r144, %r360, 30;
	shl.b32 	%r145, %r359, 2;
	add.s32 	%r361, %r144, %r145;
	shl.b32 	%r17, %r360, 2;
	shr.u32 	%r146, %r361, 31;
	shr.u32 	%r147, %r359, 30;
	add.s32 	%r18, %r146, %r147;
	setp.eq.s32	%p5, %r146, 0;
	mov.u32 	%r362, %r8;
	mov.u32 	%r363, %r17;
	@%p5 bra 	BB3_9;

	not.b32 	%r148, %r361;
	neg.s32 	%r19, %r17;
	setp.eq.s32	%p6, %r17, 0;
	selp.u32	%r149, 1, 0, %p6;
	add.s32 	%r361, %r149, %r148;
	xor.b32  	%r21, %r8, -2147483648;
	mov.u32 	%r362, %r21;
	mov.u32 	%r363, %r19;

BB3_9:
	mov.u32 	%r23, %r362;
	neg.s32 	%r150, %r18;
	setp.ne.s32	%p7, %r8, 0;
	selp.b32	%r364, %r150, %r18, %p7;
	clz.b32 	%r151, %r361;
	setp.ne.s32	%p8, %r151, 0;
	shl.b32 	%r152, %r361, %r151;
	mov.u32 	%r153, 32;
	sub.s32 	%r154, %r153, %r151;
	shr.u32 	%r155, %r363, %r154;
	add.s32 	%r156, %r155, %r152;
	selp.b32	%r157, %r156, %r361, %p8;
	mul.lo.s32 	%r158, %r157, -921707870;
	mov.u32 	%r159, -921707870;
	mul.hi.u32 	%r160, %r157, %r159;
	setp.gt.s32	%p9, %r160, 0;
	shl.b32 	%r161, %r160, 1;
	shr.u32 	%r162, %r158, 31;
	add.s32 	%r163, %r162, %r161;
	selp.b32	%r164, %r163, %r160, %p9;
	selp.b32	%r165, -1, 0, %p9;
	mov.u32 	%r166, 126;
	sub.s32 	%r167, %r166, %r151;
	add.s32 	%r168, %r167, %r165;
	shl.b32 	%r169, %r168, 23;
	add.s32 	%r170, %r164, 1;
	shr.u32 	%r171, %r170, 7;
	add.s32 	%r172, %r171, 1;
	shr.u32 	%r173, %r172, 1;
	add.s32 	%r174, %r173, %r169;
	or.b32  	%r175, %r174, %r23;
	mov.b32 	 %f229, %r175;

BB3_10:
	mul.rn.f32 	%f8, %f229, %f229;
	add.s32 	%r27, %r364, 1;
	and.b32  	%r28, %r27, 1;
	setp.eq.s32	%p10, %r28, 0;
	@%p10 bra 	BB3_12;

	mov.f32 	%f101, 0fBAB6061A;
	mov.f32 	%f102, 0f37CCF5CE;
	fma.rn.f32 	%f230, %f102, %f8, %f101;
	bra.uni 	BB3_13;

BB3_12:
	mov.f32 	%f103, 0f3C08839E;
	mov.f32 	%f104, 0fB94CA1F9;
	fma.rn.f32 	%f230, %f104, %f8, %f103;

BB3_13:
	@%p10 bra 	BB3_15;

	mov.f32 	%f105, 0f3D2AAAA5;
	fma.rn.f32 	%f106, %f230, %f8, %f105;
	mov.f32 	%f107, 0fBF000000;
	fma.rn.f32 	%f231, %f106, %f8, %f107;
	bra.uni 	BB3_16;

BB3_15:
	mov.f32 	%f108, 0fBE2AAAA3;
	fma.rn.f32 	%f109, %f230, %f8, %f108;
	mov.f32 	%f110, 0f00000000;
	fma.rn.f32 	%f231, %f109, %f8, %f110;

BB3_16:
	fma.rn.f32 	%f232, %f231, %f229, %f229;
	@%p10 bra 	BB3_18;

	mov.f32 	%f111, 0f3F800000;
	fma.rn.f32 	%f232, %f231, %f8, %f111;

BB3_18:
	and.b32  	%r176, %r27, 2;
	setp.eq.s32	%p13, %r176, 0;
	@%p13 bra 	BB3_20;

	mov.f32 	%f112, 0f00000000;
	mov.f32 	%f113, 0fBF800000;
	fma.rn.f32 	%f232, %f232, %f113, %f112;

BB3_20:
	mov.f32 	%f234, %f1;
	@%p1 bra 	BB3_22;

	mov.f32 	%f114, 0f00000000;
	mul.rn.f32 	%f234, %f1, %f114;

BB3_22:
	mul.f32 	%f115, %f234, 0f3F22F983;
	cvt.rni.s32.f32	%r372, %f115;
	cvt.rn.f32.s32	%f116, %r372;
	neg.f32 	%f117, %f116;
	fma.rn.f32 	%f119, %f117, %f95, %f234;
	fma.rn.f32 	%f121, %f117, %f97, %f119;
	fma.rn.f32 	%f236, %f117, %f99, %f121;
	abs.f32 	%f123, %f234;
	setp.leu.f32	%p15, %f123, 0f47CE4780;
	@%p15 bra 	BB3_30;

	mov.b32 	 %r30, %f234;
	shl.b32 	%r179, %r30, 8;
	or.b32  	%r31, %r179, -2147483648;
	add.u64 	%rd34, %SP, 0;
	cvta.to.local.u64 	%rd62, %rd34;
	mov.u32 	%r366, 0;
	mov.u64 	%rd61, __cudart_i2opi_f;
	mov.u32 	%r365, -6;

BB3_24:
	.pragma "nounroll";
	ld.const.u32 	%r182, [%rd61];
	// inline asm
	{
	mad.lo.cc.u32   %r180, %r182, %r31, %r366;
	madc.hi.u32     %r366, %r182, %r31,  0;
	}
	// inline asm
	st.local.u32 	[%rd62], %r180;
	add.s64 	%rd62, %rd62, 4;
	add.s64 	%rd61, %rd61, 4;
	add.s32 	%r365, %r365, 1;
	setp.ne.s32	%p16, %r365, 0;
	@%p16 bra 	BB3_24;

	bfe.u32 	%r185, %r30, 23, 8;
	add.s32 	%r186, %r185, -128;
	shr.u32 	%r187, %r186, 5;
	and.b32  	%r36, %r30, -2147483648;
	cvta.to.local.u64 	%rd36, %rd34;
	st.local.u32 	[%rd36+24], %r366;
	bfe.u32 	%r37, %r30, 23, 5;
	mov.u32 	%r188, 6;
	sub.s32 	%r189, %r188, %r187;
	mul.wide.s32 	%rd37, %r189, 4;
	add.s64 	%rd12, %rd36, %rd37;
	ld.local.u32 	%r367, [%rd12];
	ld.local.u32 	%r368, [%rd12+-4];
	setp.eq.s32	%p17, %r37, 0;
	@%p17 bra 	BB3_27;

	mov.u32 	%r190, 32;
	sub.s32 	%r191, %r190, %r37;
	shr.u32 	%r192, %r368, %r191;
	shl.b32 	%r193, %r367, %r37;
	add.s32 	%r367, %r192, %r193;
	ld.local.u32 	%r194, [%rd12+-8];
	shr.u32 	%r195, %r194, %r191;
	shl.b32 	%r196, %r368, %r37;
	add.s32 	%r368, %r195, %r196;

BB3_27:
	shr.u32 	%r197, %r368, 30;
	shl.b32 	%r198, %r367, 2;
	add.s32 	%r369, %r197, %r198;
	shl.b32 	%r45, %r368, 2;
	shr.u32 	%r199, %r369, 31;
	shr.u32 	%r200, %r367, 30;
	add.s32 	%r46, %r199, %r200;
	setp.eq.s32	%p18, %r199, 0;
	mov.u32 	%r370, %r36;
	mov.u32 	%r371, %r45;
	@%p18 bra 	BB3_29;

	not.b32 	%r201, %r369;
	neg.s32 	%r47, %r45;
	setp.eq.s32	%p19, %r45, 0;
	selp.u32	%r202, 1, 0, %p19;
	add.s32 	%r369, %r202, %r201;
	xor.b32  	%r49, %r36, -2147483648;
	mov.u32 	%r370, %r49;
	mov.u32 	%r371, %r47;

BB3_29:
	mov.u32 	%r51, %r370;
	neg.s32 	%r203, %r46;
	setp.ne.s32	%p20, %r36, 0;
	selp.b32	%r372, %r203, %r46, %p20;
	clz.b32 	%r204, %r369;
	setp.ne.s32	%p21, %r204, 0;
	shl.b32 	%r205, %r369, %r204;
	mov.u32 	%r206, 32;
	sub.s32 	%r207, %r206, %r204;
	shr.u32 	%r208, %r371, %r207;
	add.s32 	%r209, %r208, %r205;
	selp.b32	%r210, %r209, %r369, %p21;
	mul.lo.s32 	%r211, %r210, -921707870;
	mov.u32 	%r212, -921707870;
	mul.hi.u32 	%r213, %r210, %r212;
	setp.gt.s32	%p22, %r213, 0;
	shl.b32 	%r214, %r213, 1;
	shr.u32 	%r215, %r211, 31;
	add.s32 	%r216, %r215, %r214;
	selp.b32	%r217, %r216, %r213, %p22;
	selp.b32	%r218, -1, 0, %p22;
	mov.u32 	%r219, 126;
	sub.s32 	%r220, %r219, %r204;
	add.s32 	%r221, %r220, %r218;
	shl.b32 	%r222, %r221, 23;
	add.s32 	%r223, %r217, 1;
	shr.u32 	%r224, %r223, 7;
	add.s32 	%r225, %r224, 1;
	shr.u32 	%r226, %r225, 1;
	add.s32 	%r227, %r226, %r222;
	or.b32  	%r228, %r227, %r51;
	mov.b32 	 %f236, %r228;

BB3_30:
	mul.rn.f32 	%f25, %f236, %f236;
	and.b32  	%r55, %r372, 1;
	setp.eq.s32	%p23, %r55, 0;
	@%p23 bra 	BB3_32;

	mov.f32 	%f124, 0fBAB6061A;
	mov.f32 	%f125, 0f37CCF5CE;
	fma.rn.f32 	%f237, %f125, %f25, %f124;
	bra.uni 	BB3_33;

BB3_32:
	mov.f32 	%f126, 0f3C08839E;
	mov.f32 	%f127, 0fB94CA1F9;
	fma.rn.f32 	%f237, %f127, %f25, %f126;

BB3_33:
	@%p23 bra 	BB3_35;

	mov.f32 	%f128, 0f3D2AAAA5;
	fma.rn.f32 	%f129, %f237, %f25, %f128;
	mov.f32 	%f130, 0fBF000000;
	fma.rn.f32 	%f238, %f129, %f25, %f130;
	bra.uni 	BB3_36;

BB3_35:
	mov.f32 	%f131, 0fBE2AAAA3;
	fma.rn.f32 	%f132, %f237, %f25, %f131;
	mov.f32 	%f133, 0f00000000;
	fma.rn.f32 	%f238, %f132, %f25, %f133;

BB3_36:
	fma.rn.f32 	%f239, %f238, %f236, %f236;
	@%p23 bra 	BB3_38;

	mov.f32 	%f134, 0f3F800000;
	fma.rn.f32 	%f239, %f238, %f25, %f134;

BB3_38:
	and.b32  	%r229, %r372, 2;
	setp.eq.s32	%p26, %r229, 0;
	@%p26 bra 	BB3_40;

	mov.f32 	%f135, 0f00000000;
	mov.f32 	%f136, 0fBF800000;
	fma.rn.f32 	%f239, %f239, %f136, %f135;

BB3_40:
	neg.f32 	%f37, %f1;
	abs.f32 	%f38, %f37;
	setp.neu.f32	%p27, %f38, 0f7F800000;
	mov.f32 	%f246, %f37;
	@%p27 bra 	BB3_42;

	mov.f32 	%f137, 0f00000000;
	mul.rn.f32 	%f39, %f37, %f137;
	mov.f32 	%f246, %f39;

BB3_42:
	mov.f32 	%f40, %f246;
	mul.f32 	%f138, %f40, 0f3F22F983;
	cvt.rni.s32.f32	%r380, %f138;
	cvt.rn.f32.s32	%f139, %r380;
	neg.f32 	%f140, %f139;
	fma.rn.f32 	%f142, %f140, %f95, %f40;
	fma.rn.f32 	%f144, %f140, %f97, %f142;
	fma.rn.f32 	%f240, %f140, %f99, %f144;
	abs.f32 	%f146, %f40;
	setp.leu.f32	%p28, %f146, 0f47CE4780;
	@%p28 bra 	BB3_50;

	mov.b32 	 %r57, %f40;
	shl.b32 	%r232, %r57, 8;
	or.b32  	%r58, %r232, -2147483648;
	add.u64 	%rd39, %SP, 0;
	cvta.to.local.u64 	%rd64, %rd39;
	mov.u32 	%r374, 0;
	mov.u64 	%rd63, __cudart_i2opi_f;
	mov.u32 	%r373, -6;

BB3_44:
	.pragma "nounroll";
	ld.const.u32 	%r235, [%rd63];
	// inline asm
	{
	mad.lo.cc.u32   %r233, %r235, %r58, %r374;
	madc.hi.u32     %r374, %r235, %r58,  0;
	}
	// inline asm
	st.local.u32 	[%rd64], %r233;
	add.s64 	%rd64, %rd64, 4;
	add.s64 	%rd63, %rd63, 4;
	add.s32 	%r373, %r373, 1;
	setp.ne.s32	%p29, %r373, 0;
	@%p29 bra 	BB3_44;

	bfe.u32 	%r238, %r57, 23, 8;
	add.s32 	%r239, %r238, -128;
	shr.u32 	%r240, %r239, 5;
	and.b32  	%r63, %r57, -2147483648;
	cvta.to.local.u64 	%rd41, %rd39;
	st.local.u32 	[%rd41+24], %r374;
	bfe.u32 	%r64, %r57, 23, 5;
	mov.u32 	%r241, 6;
	sub.s32 	%r242, %r241, %r240;
	mul.wide.s32 	%rd42, %r242, 4;
	add.s64 	%rd18, %rd41, %rd42;
	ld.local.u32 	%r375, [%rd18];
	ld.local.u32 	%r376, [%rd18+-4];
	setp.eq.s32	%p30, %r64, 0;
	@%p30 bra 	BB3_47;

	mov.u32 	%r243, 32;
	sub.s32 	%r244, %r243, %r64;
	shr.u32 	%r245, %r376, %r244;
	shl.b32 	%r246, %r375, %r64;
	add.s32 	%r375, %r245, %r246;
	ld.local.u32 	%r247, [%rd18+-8];
	shr.u32 	%r248, %r247, %r244;
	shl.b32 	%r249, %r376, %r64;
	add.s32 	%r376, %r248, %r249;

BB3_47:
	shr.u32 	%r250, %r376, 30;
	shl.b32 	%r251, %r375, 2;
	add.s32 	%r377, %r250, %r251;
	shl.b32 	%r72, %r376, 2;
	shr.u32 	%r252, %r377, 31;
	shr.u32 	%r253, %r375, 30;
	add.s32 	%r73, %r252, %r253;
	setp.eq.s32	%p31, %r252, 0;
	mov.u32 	%r378, %r63;
	mov.u32 	%r379, %r72;
	@%p31 bra 	BB3_49;

	not.b32 	%r254, %r377;
	neg.s32 	%r74, %r72;
	setp.eq.s32	%p32, %r72, 0;
	selp.u32	%r255, 1, 0, %p32;
	add.s32 	%r377, %r255, %r254;
	xor.b32  	%r76, %r63, -2147483648;
	mov.u32 	%r378, %r76;
	mov.u32 	%r379, %r74;

BB3_49:
	mov.u32 	%r78, %r378;
	neg.s32 	%r256, %r73;
	setp.ne.s32	%p33, %r63, 0;
	selp.b32	%r380, %r256, %r73, %p33;
	clz.b32 	%r257, %r377;
	setp.ne.s32	%p34, %r257, 0;
	shl.b32 	%r258, %r377, %r257;
	mov.u32 	%r259, 32;
	sub.s32 	%r260, %r259, %r257;
	shr.u32 	%r261, %r379, %r260;
	add.s32 	%r262, %r261, %r258;
	selp.b32	%r263, %r262, %r377, %p34;
	mul.lo.s32 	%r264, %r263, -921707870;
	mov.u32 	%r265, -921707870;
	mul.hi.u32 	%r266, %r263, %r265;
	setp.gt.s32	%p35, %r266, 0;
	shl.b32 	%r267, %r266, 1;
	shr.u32 	%r268, %r264, 31;
	add.s32 	%r269, %r268, %r267;
	selp.b32	%r270, %r269, %r266, %p35;
	selp.b32	%r271, -1, 0, %p35;
	mov.u32 	%r272, 126;
	sub.s32 	%r273, %r272, %r257;
	add.s32 	%r274, %r273, %r271;
	shl.b32 	%r275, %r274, 23;
	add.s32 	%r276, %r270, 1;
	shr.u32 	%r277, %r276, 7;
	add.s32 	%r278, %r277, 1;
	shr.u32 	%r279, %r278, 1;
	add.s32 	%r280, %r279, %r275;
	or.b32  	%r281, %r280, %r78;
	mov.b32 	 %f240, %r281;

BB3_50:
	mul.rn.f32 	%f44, %f240, %f240;
	add.s32 	%r82, %r380, 1;
	and.b32  	%r83, %r82, 1;
	setp.eq.s32	%p36, %r83, 0;
	@%p36 bra 	BB3_52;

	mov.f32 	%f147, 0fBAB6061A;
	mov.f32 	%f148, 0f37CCF5CE;
	fma.rn.f32 	%f241, %f148, %f44, %f147;
	bra.uni 	BB3_53;

BB3_52:
	mov.f32 	%f149, 0f3C08839E;
	mov.f32 	%f150, 0fB94CA1F9;
	fma.rn.f32 	%f241, %f150, %f44, %f149;

BB3_53:
	@%p36 bra 	BB3_55;

	mov.f32 	%f151, 0f3D2AAAA5;
	fma.rn.f32 	%f152, %f241, %f44, %f151;
	mov.f32 	%f153, 0fBF000000;
	fma.rn.f32 	%f242, %f152, %f44, %f153;
	bra.uni 	BB3_56;

BB3_55:
	mov.f32 	%f154, 0fBE2AAAA3;
	fma.rn.f32 	%f155, %f241, %f44, %f154;
	mov.f32 	%f156, 0f00000000;
	fma.rn.f32 	%f242, %f155, %f44, %f156;

BB3_56:
	fma.rn.f32 	%f243, %f242, %f240, %f240;
	@%p36 bra 	BB3_58;

	mov.f32 	%f157, 0f3F800000;
	fma.rn.f32 	%f243, %f242, %f44, %f157;

BB3_58:
	and.b32  	%r282, %r82, 2;
	setp.eq.s32	%p39, %r282, 0;
	@%p39 bra 	BB3_60;

	mov.f32 	%f158, 0f00000000;
	mov.f32 	%f159, 0fBF800000;
	fma.rn.f32 	%f243, %f243, %f159, %f158;

BB3_60:
	mov.f32 	%f245, %f37;
	@%p27 bra 	BB3_62;

	mov.f32 	%f160, 0f00000000;
	mul.rn.f32 	%f245, %f37, %f160;

BB3_62:
	mul.f32 	%f161, %f245, 0f3F22F983;
	cvt.rni.s32.f32	%r388, %f161;
	cvt.rn.f32.s32	%f162, %r388;
	neg.f32 	%f163, %f162;
	fma.rn.f32 	%f165, %f163, %f95, %f245;
	fma.rn.f32 	%f167, %f163, %f97, %f165;
	fma.rn.f32 	%f247, %f163, %f99, %f167;
	abs.f32 	%f169, %f245;
	setp.leu.f32	%p41, %f169, 0f47CE4780;
	@%p41 bra 	BB3_70;

	mov.b32 	 %r85, %f245;
	shl.b32 	%r285, %r85, 8;
	or.b32  	%r86, %r285, -2147483648;
	add.u64 	%rd44, %SP, 0;
	cvta.to.local.u64 	%rd66, %rd44;
	mov.u32 	%r382, 0;
	mov.u64 	%rd65, __cudart_i2opi_f;
	mov.u32 	%r381, -6;

BB3_64:
	.pragma "nounroll";
	ld.const.u32 	%r288, [%rd65];
	// inline asm
	{
	mad.lo.cc.u32   %r286, %r288, %r86, %r382;
	madc.hi.u32     %r382, %r288, %r86,  0;
	}
	// inline asm
	st.local.u32 	[%rd66], %r286;
	add.s64 	%rd66, %rd66, 4;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r381, %r381, 1;
	setp.ne.s32	%p42, %r381, 0;
	@%p42 bra 	BB3_64;

	bfe.u32 	%r291, %r85, 23, 8;
	add.s32 	%r292, %r291, -128;
	shr.u32 	%r293, %r292, 5;
	and.b32  	%r91, %r85, -2147483648;
	cvta.to.local.u64 	%rd46, %rd44;
	st.local.u32 	[%rd46+24], %r382;
	bfe.u32 	%r92, %r85, 23, 5;
	mov.u32 	%r294, 6;
	sub.s32 	%r295, %r294, %r293;
	mul.wide.s32 	%rd47, %r295, 4;
	add.s64 	%rd24, %rd46, %rd47;
	ld.local.u32 	%r383, [%rd24];
	ld.local.u32 	%r384, [%rd24+-4];
	setp.eq.s32	%p43, %r92, 0;
	@%p43 bra 	BB3_67;

	mov.u32 	%r296, 32;
	sub.s32 	%r297, %r296, %r92;
	shr.u32 	%r298, %r384, %r297;
	shl.b32 	%r299, %r383, %r92;
	add.s32 	%r383, %r298, %r299;
	ld.local.u32 	%r300, [%rd24+-8];
	shr.u32 	%r301, %r300, %r297;
	shl.b32 	%r302, %r384, %r92;
	add.s32 	%r384, %r301, %r302;

BB3_67:
	shr.u32 	%r303, %r384, 30;
	shl.b32 	%r304, %r383, 2;
	add.s32 	%r385, %r303, %r304;
	shl.b32 	%r100, %r384, 2;
	shr.u32 	%r305, %r385, 31;
	shr.u32 	%r306, %r383, 30;
	add.s32 	%r101, %r305, %r306;
	setp.eq.s32	%p44, %r305, 0;
	mov.u32 	%r386, %r91;
	mov.u32 	%r387, %r100;
	@%p44 bra 	BB3_69;

	not.b32 	%r307, %r385;
	neg.s32 	%r102, %r100;
	setp.eq.s32	%p45, %r100, 0;
	selp.u32	%r308, 1, 0, %p45;
	add.s32 	%r385, %r308, %r307;
	xor.b32  	%r104, %r91, -2147483648;
	mov.u32 	%r386, %r104;
	mov.u32 	%r387, %r102;

BB3_69:
	mov.u32 	%r106, %r386;
	neg.s32 	%r309, %r101;
	setp.ne.s32	%p46, %r91, 0;
	selp.b32	%r388, %r309, %r101, %p46;
	clz.b32 	%r310, %r385;
	setp.ne.s32	%p47, %r310, 0;
	shl.b32 	%r311, %r385, %r310;
	mov.u32 	%r312, 32;
	sub.s32 	%r313, %r312, %r310;
	shr.u32 	%r314, %r387, %r313;
	add.s32 	%r315, %r314, %r311;
	selp.b32	%r316, %r315, %r385, %p47;
	mul.lo.s32 	%r317, %r316, -921707870;
	mov.u32 	%r318, -921707870;
	mul.hi.u32 	%r319, %r316, %r318;
	setp.gt.s32	%p48, %r319, 0;
	shl.b32 	%r320, %r319, 1;
	shr.u32 	%r321, %r317, 31;
	add.s32 	%r322, %r321, %r320;
	selp.b32	%r323, %r322, %r319, %p48;
	selp.b32	%r324, -1, 0, %p48;
	mov.u32 	%r325, 126;
	sub.s32 	%r326, %r325, %r310;
	add.s32 	%r327, %r326, %r324;
	shl.b32 	%r328, %r327, 23;
	add.s32 	%r329, %r323, 1;
	shr.u32 	%r330, %r329, 7;
	add.s32 	%r331, %r330, 1;
	shr.u32 	%r332, %r331, 1;
	add.s32 	%r333, %r332, %r328;
	or.b32  	%r334, %r333, %r106;
	mov.b32 	 %f247, %r334;

BB3_70:
	mul.rn.f32 	%f61, %f247, %f247;
	and.b32  	%r110, %r388, 1;
	setp.eq.s32	%p49, %r110, 0;
	@%p49 bra 	BB3_72;

	mov.f32 	%f170, 0fBAB6061A;
	mov.f32 	%f171, 0f37CCF5CE;
	fma.rn.f32 	%f248, %f171, %f61, %f170;
	bra.uni 	BB3_73;

BB3_72:
	mov.f32 	%f172, 0f3C08839E;
	mov.f32 	%f173, 0fB94CA1F9;
	fma.rn.f32 	%f248, %f173, %f61, %f172;

BB3_73:
	@%p49 bra 	BB3_75;

	mov.f32 	%f174, 0f3D2AAAA5;
	fma.rn.f32 	%f175, %f248, %f61, %f174;
	mov.f32 	%f176, 0fBF000000;
	fma.rn.f32 	%f249, %f175, %f61, %f176;
	bra.uni 	BB3_76;

BB3_75:
	mov.f32 	%f177, 0fBE2AAAA3;
	fma.rn.f32 	%f178, %f248, %f61, %f177;
	mov.f32 	%f179, 0f00000000;
	fma.rn.f32 	%f249, %f178, %f61, %f179;

BB3_76:
	fma.rn.f32 	%f250, %f249, %f247, %f247;
	@%p49 bra 	BB3_78;

	mov.f32 	%f180, 0f3F800000;
	fma.rn.f32 	%f250, %f249, %f61, %f180;

BB3_78:
	and.b32  	%r335, %r388, 2;
	setp.eq.s32	%p52, %r335, 0;
	@%p52 bra 	BB3_80;

	mov.f32 	%f181, 0f00000000;
	mov.f32 	%f182, 0fBF800000;
	fma.rn.f32 	%f250, %f250, %f182, %f181;

BB3_80:
	setp.lt.u32	%p53, %r117, %r112;
	setp.lt.u32	%p54, %r121, %r113;
	and.pred  	%p55, %p53, %p54;
	@!%p55 bra 	BB3_82;
	bra.uni 	BB3_81;

BB3_81:
	cvta.to.global.u64 	%rd48, %rd26;
	mad.lo.s32 	%r352, %r121, %r111, %r117;
	cvta.to.global.u64 	%rd49, %rd25;
	mul.wide.u32 	%rd50, %r352, 8;
	add.s64 	%rd51, %rd49, %rd50;
	sub.s32 	%r353, %r112, %r117;
	sub.s32 	%r354, %r113, %r121;
	mad.lo.s32 	%r355, %r354, %r111, %r353;
	mul.wide.u32 	%rd52, %r355, 8;
	add.s64 	%rd53, %rd49, %rd52;
	cvta.to.global.u64 	%rd54, %rd27;
	add.s64 	%rd55, %rd54, %rd50;
	add.s64 	%rd56, %rd54, %rd52;
	ld.global.v2.f32 	{%f183, %f184}, [%rd51];
	mul.f32 	%f187, %f232, %f183;
	mul.f32 	%f188, %f239, %f184;
	sub.f32 	%f189, %f187, %f188;
	mul.f32 	%f190, %f232, %f184;
	fma.rn.f32 	%f191, %f239, %f183, %f190;
	ld.global.v2.f32 	{%f192, %f193}, [%rd53];
	mul.f32 	%f196, %f250, %f193;
	fma.rn.f32 	%f197, %f243, %f192, %f196;
	mul.f32 	%f198, %f250, %f192;
	mul.f32 	%f199, %f243, %f193;
	sub.f32 	%f200, %f198, %f199;
	add.f32 	%f201, %f189, %f197;
	add.f32 	%f202, %f191, %f200;
	ld.global.v2.f32 	{%f203, %f204}, [%rd55];
	mul.f32 	%f207, %f232, %f203;
	mul.f32 	%f208, %f239, %f204;
	sub.f32 	%f209, %f207, %f208;
	mul.f32 	%f210, %f232, %f204;
	fma.rn.f32 	%f211, %f239, %f203, %f210;
	ld.global.v2.f32 	{%f212, %f213}, [%rd56];
	mul.f32 	%f216, %f250, %f213;
	fma.rn.f32 	%f217, %f243, %f212, %f216;
	mul.f32 	%f218, %f250, %f212;
	mul.f32 	%f219, %f243, %f213;
	sub.f32 	%f220, %f218, %f219;
	add.f32 	%f221, %f209, %f217;
	add.f32 	%f222, %f211, %f220;
	mul.f32 	%f223, %f221, 0f3DCCCCCD;
	mul.f32 	%f224, %f222, 0f3DCCCCCD;
	mad.lo.s32 	%r356, %r121, %r112, %r117;
	sub.f32 	%f225, %f223, %f201;
	sub.f32 	%f226, %f224, %f202;
	mul.wide.u32 	%rd57, %r356, 8;
	add.s64 	%rd58, %rd48, %rd57;
	fma.rn.f32 	%f227, %f226, %f73, %f202;
	fma.rn.f32 	%f228, %f225, %f73, %f201;
	st.global.v2.f32 	[%rd58], {%f228, %f227};

BB3_82:
	ret;
}

	// .globl	updateHeightmapKernel
.visible .entry updateHeightmapKernel(
	.param .u64 updateHeightmapKernel_param_0,
	.param .u64 updateHeightmapKernel_param_1,
	.param .u32 updateHeightmapKernel_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [updateHeightmapKernel_param_0];
	ld.param.u64 	%rd2, [updateHeightmapKernel_param_1];
	ld.param.u32 	%r1, [updateHeightmapKernel_param_2];
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r3, %r2, %r4;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r7, %r6, %r8;
	mad.lo.s32 	%r10, %r9, %r1, %r5;
	add.s32 	%r11, %r9, %r5;
	and.b32  	%r12, %r11, 1;
	setp.eq.b32	%p1, %r12, 1;
	selp.f32	%f1, 0fBF800000, 0f3F800000, %p1;
	mul.wide.u32 	%rd5, %r10, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f2, [%rd6];
	mul.f32 	%f3, %f2, %f1;
	mul.wide.u32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd3, %rd7;
	st.global.f32 	[%rd8], %f3;
	ret;
}

	// .globl	calculateSlopeKernel
.visible .entry calculateSlopeKernel(
	.param .u64 calculateSlopeKernel_param_0,
	.param .u64 calculateSlopeKernel_param_1,
	.param .u32 calculateSlopeKernel_param_2,
	.param .u32 calculateSlopeKernel_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [calculateSlopeKernel_param_0];
	ld.param.u64 	%rd2, [calculateSlopeKernel_param_1];
	ld.param.u32 	%r2, [calculateSlopeKernel_param_2];
	ld.param.u32 	%r3, [calculateSlopeKernel_param_3];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mad.lo.s32 	%r1, %r11, %r2, %r7;
	setp.ne.s32	%p1, %r7, 0;
	setp.ne.s32	%p2, %r11, 0;
	and.pred  	%p3, %p1, %p2;
	add.s32 	%r12, %r2, -1;
	setp.lt.u32	%p4, %r7, %r12;
	and.pred  	%p5, %p3, %p4;
	add.s32 	%r13, %r3, -1;
	setp.lt.u32	%p6, %r11, %r13;
	and.pred  	%p7, %p5, %p6;
	mov.f32 	%f12, 0f00000000;
	mov.f32 	%f11, %f12;
	@!%p7 bra 	BB5_2;
	bra.uni 	BB5_1;

BB5_1:
	cvta.to.global.u64 	%rd3, %rd1;
	add.s32 	%r14, %r1, 1;
	mul.wide.u32 	%rd4, %r14, 4;
	add.s64 	%rd5, %rd3, %rd4;
	add.s32 	%r15, %r1, -1;
	mul.wide.u32 	%rd6, %r15, 4;
	add.s64 	%rd7, %rd3, %rd6;
	ld.global.f32 	%f7, [%rd7];
	ld.global.f32 	%f8, [%rd5];
	sub.f32 	%f12, %f8, %f7;
	add.s32 	%r16, %r1, %r2;
	mul.wide.u32 	%rd8, %r16, 4;
	add.s64 	%rd9, %rd3, %rd8;
	sub.s32 	%r17, %r1, %r2;
	mul.wide.u32 	%rd10, %r17, 4;
	add.s64 	%rd11, %rd3, %rd10;
	ld.global.f32 	%f9, [%rd11];
	ld.global.f32 	%f10, [%rd9];
	sub.f32 	%f11, %f10, %f9;

BB5_2:
	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r1, 8;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.v2.f32 	[%rd14], {%f12, %f11};
	ret;
}


